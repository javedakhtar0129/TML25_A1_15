{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T11:50:08.766534Z",
     "start_time": "2025-05-23T11:50:05.083968Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfcb6bceb1ea99c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T11:50:29.807459Z",
     "start_time": "2025-05-23T11:50:29.800412Z"
    }
   },
   "outputs": [],
   "source": [
    "### LOADING THE MODEL\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecba666babb3f3d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T11:50:54.530321Z",
     "start_time": "2025-05-23T11:50:54.345334Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Add this as a transofrmation to pre-process the images\n",
    "mean = [0.2980, 0.2962, 0.2987]\n",
    "std = [0.2886, 0.2875, 0.2889]\n",
    "\n",
    "model = resnet18(weights=False)\n",
    "model.fc = torch.nn.Linear(512, 44)\n",
    "\n",
    "ckpt = torch.load(\"01_MIA.pt\", map_location=\"cpu\",weights_only=False)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ac3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, pt_file=None, transform=None):\n",
    "        self.ids = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        # Optionally load from pt file if provided\n",
    "        if pt_file is not None:\n",
    "            data = torch.load(pt_file)\n",
    "            self.ids = data[\"ids\"]\n",
    "            self.imgs = data[\"imgs\"]\n",
    "            self.labels = data[\"labels\"]\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int]:\n",
    "        id_ = self.ids[index]\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return id_, img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "class MembershipDataset(TaskDataset):\n",
    "    def __init__(self, pt_file=None, transform=None):\n",
    "        super().__init__(pt_file, transform)\n",
    "        self.membership = []\n",
    "        if pt_file is not None:\n",
    "            data = torch.load(pt_file)\n",
    "            # Only load membership if present (for public data)\n",
    "            self.membership = data.get(\"membership\", [0]*len(self.ids))\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int, int]:\n",
    "        id_, img, label = super().__getitem__(index)\n",
    "        membership = self.membership[index]\n",
    "        return id_, img, label, membership\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "819c0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccb32267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264399,\n",
       " tensor([[[-0.6657, -0.5570, -0.4483,  ..., -0.4347, -0.3667, -0.2309],\n",
       "          [-0.6929, -0.6113, -0.5162,  ..., -0.4890, -0.4483, -0.2852],\n",
       "          [-0.6929, -0.6657, -0.6385,  ..., -0.5842, -0.5298, -0.4211],\n",
       "          ...,\n",
       "          [-0.4483, -0.4890, -0.5162,  ..., -0.3260, -0.3396, -0.3939],\n",
       "          [-0.5026, -0.4890, -0.5570,  ..., -0.3396, -0.3532, -0.4619],\n",
       "          [-0.5162, -0.5570, -0.6113,  ..., -0.4755, -0.4890, -0.5162]],\n",
       " \n",
       "         [[-0.6620, -0.5529, -0.4437,  ..., -0.4301, -0.3619, -0.2255],\n",
       "          [-0.6893, -0.6074, -0.5119,  ..., -0.4847, -0.4437, -0.2800],\n",
       "          [-0.6893, -0.6620, -0.6347,  ..., -0.5801, -0.5256, -0.4165],\n",
       "          ...,\n",
       "          [-0.4437, -0.4847, -0.5119,  ..., -0.3210, -0.3346, -0.3892],\n",
       "          [-0.4983, -0.4847, -0.5529,  ..., -0.3346, -0.3482, -0.4574],\n",
       "          [-0.5119, -0.5529, -0.6074,  ..., -0.4710, -0.4847, -0.5119]],\n",
       " \n",
       "         [[-0.6674, -0.5588, -0.4502,  ..., -0.4367, -0.3688, -0.2330],\n",
       "          [-0.6946, -0.6131, -0.5181,  ..., -0.4910, -0.4502, -0.2873],\n",
       "          [-0.6946, -0.6674, -0.6403,  ..., -0.5860, -0.5317, -0.4231],\n",
       "          ...,\n",
       "          [-0.4502, -0.4910, -0.5181,  ..., -0.3281, -0.3416, -0.3959],\n",
       "          [-0.5045, -0.4910, -0.5588,  ..., -0.3416, -0.3552, -0.4638],\n",
       "          [-0.5181, -0.5588, -0.6131,  ..., -0.4774, -0.4910, -0.5181]]]),\n",
       " 17,\n",
       " 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.serialization.add_safe_globals([MembershipDataset])\n",
    "\n",
    "public_data: MembershipDataset = torch.load(\"pub.pt\",weights_only=False)\n",
    "public_data.transform = transform\n",
    "\n",
    "public_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a13be7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55061,\n",
       " tensor([[[1.9025, 1.9297, 1.8889,  ..., 1.7258, 1.8074, 1.8617],\n",
       "          [1.7938, 1.8345, 1.8345,  ..., 1.8481, 1.9297, 1.8617],\n",
       "          [1.8889, 1.8617, 1.8617,  ..., 1.8481, 1.8210, 1.7530],\n",
       "          ...,\n",
       "          [1.8617, 1.8617, 1.8753,  ..., 1.2910, 1.6035, 1.6579],\n",
       "          [1.8210, 1.8481, 1.8753,  ..., 1.7394, 1.8481, 1.8889],\n",
       "          [1.2638, 1.5628, 1.7666,  ..., 1.7666, 1.7258, 1.7123]],\n",
       " \n",
       "         [[0.9203, 0.9067, 0.7975,  ..., 0.5247, 0.6475, 0.7157],\n",
       "          [0.5111, 0.5111, 0.4565,  ..., 0.6338, 0.7020, 0.6202],\n",
       "          [0.5656, 0.5793, 0.5929,  ..., 0.6611, 0.6338, 0.5793],\n",
       "          ...,\n",
       "          [0.7430, 0.6748, 0.6748,  ..., 0.3201, 0.5656, 0.6066],\n",
       "          [0.6475, 0.6475, 0.6338,  ..., 0.5247, 0.6066, 0.6338],\n",
       "          [0.2656, 0.4156, 0.5247,  ..., 0.4838, 0.4156, 0.4156]],\n",
       " \n",
       "         [[1.5452, 1.5180, 1.4230,  ..., 1.2330, 1.3416, 1.3958],\n",
       "          [1.2330, 1.2330, 1.1922,  ..., 1.3416, 1.3958, 1.3416],\n",
       "          [1.2737, 1.2737, 1.2873,  ..., 1.3551, 1.3416, 1.2873],\n",
       "          ...,\n",
       "          [1.4094, 1.3551, 1.3416,  ..., 1.0972, 1.2601, 1.2737],\n",
       "          [1.3280, 1.3416, 1.3280,  ..., 1.2601, 1.3144, 1.3416],\n",
       "          [1.0565, 1.1651, 1.2330,  ..., 1.2058, 1.1515, 1.1651]]]),\n",
       " 11,\n",
       " None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_data: MembershipDataset = torch.load(\"priv_out.pt\",weights_only=False)\n",
    "private_data.transform = transform\n",
    "private_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec2640eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, input_tensor, label, device='cpu'):\n",
    "    model.zero_grad()\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "    label_tensor = torch.tensor([label]).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    output = model(input_tensor)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    gradients = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients.append(param.grad.detach().cpu().flatten())\n",
    "    gradient_vector = torch.cat(gradients).numpy()\n",
    "    return gradient_vector, loss.item() \n",
    "\n",
    "def aggregate_gradient(grad_vector, num_chunks=10):\n",
    "    grad_chunks = np.array_split(grad_vector, num_chunks)\n",
    "    agg = np.array([np.linalg.norm(chunk) for chunk in grad_chunks])\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a62d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHADOW MODEL\n",
    "class ShadowListDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples  # list of (img_id, img, label, membership)\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, img, label, membership = self.samples[idx]\n",
    "        return img_id, img, label, membership\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def build_and_train_shadow_model(shadow_train_data, device=\"cpu\", epochs=10, lr=1e-3,batch_size=16):\n",
    "    model = resnet18(weights=None)\n",
    "    model.fc = torch.nn.Linear(512, 44) \n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    dataset = ShadowListDataset(shadow_train_data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch in loader:\n",
    "            img_ids, imgs, labels, memberships = batch\n",
    "            imgs = imgs.to(device)        # [B, 3, H, W]\n",
    "            labels = labels.to(device)    # [B]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(imgs)          # [B, 44]\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training shadow model 1/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [18:24<00:00, 110.43s/it]\n",
      "Shadow 1 Train: 100%|█████████████████████| 12000/12000 [03:47<00:00, 52.66it/s]\n",
      "Shadow 1 Test: 100%|████████████████████████| 8000/8000 [02:22<00:00, 55.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training shadow model 2/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [20:03<00:00, 120.38s/it]\n",
      "Shadow 2 Train:  35%|███████▊              | 4252/12000 [01:23<02:19, 55.54it/s]"
     ]
    }
   ],
   "source": [
    "num_shadows = 2  # You can increase for stronger attack\n",
    "shadow_attack_X = []\n",
    "shadow_attack_y = []\n",
    "shadow_losses_in = dict()   # For LiRA (losses for \"in\" cases)\n",
    "shadow_losses_out = dict()  # For LiRA (losses for \"out\" cases)\n",
    "\n",
    "all_samples = [(img_id, img, label, membership) for img_id, img, label, membership in public_data]\n",
    "num_samples = len(all_samples)\n",
    "\n",
    "for s in range(num_shadows):\n",
    "    print(f\"\\nTraining shadow model {s+1}/{num_shadows}...\")\n",
    "    # Split public data randomly: 50% train (members), 50% test (non-members)\n",
    "    indices = np.arange(num_samples)\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=0.4, random_state=s)\n",
    "    shadow_train = [all_samples[i] for i in train_idx]\n",
    "    shadow_test = [all_samples[i] for i in test_idx]\n",
    "\n",
    "    # Train a shadow model (same as your target, ideally new initialization)\n",
    "    # For demonstration, let's assume you have a train_model function or just reload a new model here\n",
    "    shadow_model = build_and_train_shadow_model(shadow_train, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                                                epochs=10, lr=1e-3,batch_size=16)\n",
    "\n",
    "    # Features for attack model (member = 1)\n",
    "    for img_id, img, label, membership in tqdm(shadow_train, desc=f\"Shadow {s+1} Train\"):\n",
    "        grad_vec, loss_val = compute_gradients(shadow_model, img, label)\n",
    "        agg_feat = aggregate_gradient(grad_vec)\n",
    "        shadow_attack_X.append(agg_feat)\n",
    "        shadow_attack_y.append(1)\n",
    "        # Store losses for LiRA\n",
    "        shadow_losses_in.setdefault(img_id, []).append(loss_val)\n",
    "\n",
    "    # Features for attack model (non-member = 0)\n",
    "    for img_id, img, label, membership in tqdm(shadow_test, desc=f\"Shadow {s+1} Test\"):\n",
    "        grad_vec, loss_val = compute_gradients(shadow_model, img, label)\n",
    "        agg_feat = aggregate_gradient(grad_vec)\n",
    "        shadow_attack_X.append(agg_feat)\n",
    "        shadow_attack_y.append(0)\n",
    "        shadow_losses_out.setdefault(img_id, []).append(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = [], []\n",
    "\n",
    "# for img_id,img,label,membership in tqdm(public_data, desc='Extracting Public Features'):\n",
    "#     grad_vec = compute_gradients(model, img, label)\n",
    "#     agg_feat = aggregate_gradient(grad_vec)\n",
    "#     X_train.append(agg_feat)\n",
    "#     y_train.append(membership)\n",
    "\n",
    "# X_train = np.vstack(X_train)\n",
    "# y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "# attack_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200)\n",
    "# attack_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_features, private_ids, private_losses = [], [], []\n",
    "\n",
    "for img_id, img, label, membership in tqdm(private_data, desc='Extracting Private Features'):\n",
    "    grad_vec, loss_val = compute_gradients(model, img, label)  \n",
    "    agg_feat = aggregate_gradient(grad_vec)\n",
    "    private_features.append(agg_feat)\n",
    "    private_ids.append(img_id)\n",
    "    private_losses.append(loss_val)\n",
    "\n",
    "X_private = np.vstack(private_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dafd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_attack_X = np.vstack(shadow_attack_X)\n",
    "shadow_attack_y = np.array(shadow_attack_y)\n",
    "\n",
    "# np.save('shadow_attack_X.npy', shadow_attack_X)\n",
    "# np.save('shadow_attack_y.npy', shadow_attack_y)\n",
    "# attack_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "# attack_model = RandomForestClassifier(n_estimators=100)\n",
    "# attack_model.fit(shadow_attack_X, shadow_attack_y)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def tpr_at_fpr(y_true, y_scores, fpr_level=0.05):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    idx = np.where(fpr <= fpr_level)[0]\n",
    "    if len(idx) == 0:\n",
    "        return 0.0\n",
    "    return float(tpr[idx[-1]])\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=0),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=0),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200, random_state=0)\n",
    "}\n",
    "\n",
    "results = []\n",
    "model_shadow_scores = {}\n",
    "model_metrics = {}\n",
    "\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\nTraining {name} attack model...\")\n",
    "    clf.fit(shadow_attack_X, shadow_attack_y)\n",
    "    shadow_scores = clf.predict_proba(shadow_attack_X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(shadow_attack_y, shadow_scores)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    tpr_05 = tpr_at_fpr(shadow_attack_y, shadow_scores, fpr_level=0.05)\n",
    "    \n",
    "    # Store scores and metrics\n",
    "    model_shadow_scores[name] = {\n",
    "        'scores': shadow_scores,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds\n",
    "    }\n",
    "    model_metrics[name] = {\n",
    "        \"AUC\": auc_score,\n",
    "        \"TPR@FPR=0.05\": tpr_05\n",
    "    }\n",
    "    \n",
    "    #---- PRIVATE DATA PREDICTION (for server submission) ----\n",
    "    membership_scores = clf.predict_proba(X_private)[:, 1]\n",
    "    df = pd.DataFrame({\n",
    "        \"ids\": private_ids,\n",
    "        \"score\": membership_scores\n",
    "    })\n",
    "    out_csv = f\"test_{name}.csv\"\n",
    "    df.to_csv(out_csv, index=None)\n",
    "    \n",
    "    print(f\"Saved private membership scores for server to: {out_csv}\")\n",
    "    print(f\"{name} shadow attack AUC: {auc_score:.4f} | TPR@FPR=0.05: {tpr_05:.4f}\")\n",
    "\n",
    "print(\"\\nAll models evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fe4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shadow_attack_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6b015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_ids[10],private_features[10],private_losses[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff65efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c71f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_scores = attack_model.predict_proba(X_private)[:, 1]  # Probability of being member\n",
    "\n",
    "membership_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "all_in_losses = []\n",
    "all_out_losses = []\n",
    "for losses in shadow_losses_in.values():\n",
    "    all_in_losses.extend(losses)\n",
    "for losses in shadow_losses_out.values():\n",
    "    all_out_losses.extend(losses)\n",
    "\n",
    "kde_in = gaussian_kde(all_in_losses) if len(all_in_losses) > 1 else None\n",
    "kde_out = gaussian_kde(all_out_losses) if len(all_out_losses) > 1 else None\n",
    "\n",
    "lira_scores = []\n",
    "for loss_val in private_losses:  # just loop through losses (no img_id lookup needed)\n",
    "    if kde_in is None or kde_out is None:\n",
    "        lira_scores.append(0.0)\n",
    "        continue\n",
    "    p_in = kde_in.evaluate(loss_val)[0]\n",
    "    p_out = kde_out.evaluate(loss_val)[0]\n",
    "    lira_score = p_in / (p_out + 1e-12)\n",
    "    lira_scores.append(lira_score)\n",
    "print(lira_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lira_scores = np.array(lira_scores)\n",
    "membership_confidences_lira = lira_scores / (1 + lira_scores)\n",
    "membership_confidences_lira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ids\": private_ids,\n",
    "    \"score\": membership_confidences_lira,\n",
    "})\n",
    "df.to_csv(\"test_Lira.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit to server\n",
    "response = requests.post(\"http://34.122.51.94:9090/mia\", files={\"file\": open(\"test_MLP.csv\", \"rb\")}, headers={\"token\": \"50407833\"})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac63435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'TPR@FPR=0.05': 0.043, 'AUC': 0.5013481666666666}\n",
    "# {'TPR@FPR=0.05': 0.08366666666666667, 'AUC': 0.6373154999999999}\n",
    "# LiRa {'TPR@FPR=0.05': 0.058, 'AUC': 0.6262959444444445}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(public_data.membership, membership_scores)\n",
    "fpr, tpr, thresholds = roc_curve(public_data.membership, membership_scores)\n",
    "tpr_at_fpr_005 = tpr[fpr <= 0.05].max()\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"TPR@FPR=0.05: {tpr_at_fpr_005:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24579cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC: 0.4982\n",
    "# TPR@FPR=0.05: 0.040\n",
    "\n",
    "len(shadow_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Plot Score Distribution: Member vs Non-member for each model\n",
    "plt.figure(figsize=(10, 5))\n",
    "y_true = np.array(shadow_attack_y)\n",
    "for name, res in model_shadow_scores.items():\n",
    "    y_score = res['scores']\n",
    "    plt.hist(y_score[y_true == 1], bins=30, alpha=0.5, label=f\"{name} Members\", histtype='stepfilled', linewidth=2)\n",
    "    plt.hist(y_score[y_true == 0], bins=30, alpha=0.3, label=f\"{name} Non-members\", histtype='step', linewidth=2)\n",
    "plt.xlabel(\"Predicted Membership Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Attack Model Scores: Member vs Non-member (All Models, Shadow Set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plot ROC curves for each model\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, res in model_shadow_scores.items():\n",
    "    plt.plot(res['fpr'], res['tpr'], label=f\"{name} (AUC={model_metrics[name]['AUC']:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random (AUC=0.5)\")\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curves for Membership Attack Models (Shadow Set)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, plot all models' score distributions on one plot (not split by membership)\n",
    "plt.figure(figsize=(8, 5))\n",
    "for name, res in model_shadow_scores.items():\n",
    "    plt.hist(res['scores'], bins=30, alpha=0.5, label=name)\n",
    "plt.xlabel(\"Predicted Membership Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Attack Model Score Distributions (Shadow Set, All Models)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02deb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the model is a overfit\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Get model into eval mode\n",
    "model.eval()\n",
    "\n",
    "# 2. Compute predictions and loss on public data\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_losses = []\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for img_id, img, label, membership in tqdm(public_data, desc=\"Evaluating Public Dataset\"):\n",
    "    img = img.unsqueeze(0)  # Add batch dimension\n",
    "    output = model(img)\n",
    "    pred = output.argmax(dim=1).item()\n",
    "    loss = criterion(output, torch.tensor([label]))\n",
    "    all_labels.append(label)\n",
    "    all_preds.append(pred)\n",
    "    all_losses.append(loss.item())\n",
    "\n",
    "# 3. Compute accuracy\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "accuracy = np.mean(all_preds == all_labels)\n",
    "print(f\"Public Dataset Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4. Visualize loss distribution\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(all_losses, bins=30, color='royalblue', alpha=0.7)\n",
    "plt.xlabel(\"Cross Entropy Loss\")\n",
    "plt.ylabel(\"Sample Count\")\n",
    "plt.title(\"Loss Distribution on Public Dataset\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Public Loss: {np.mean(all_losses):.4f}\")\n",
    "\n",
    "# 5. (Optional) Print a few predictions and losses\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}: Label={all_labels[i]}, Pred={all_preds[i]}, Loss={all_losses[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8055a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_labels = []\n",
    "private_preds = []\n",
    "private_losses = []\n",
    "\n",
    "for img_id, img, label, membership in tqdm(private_data, desc=\"Evaluating Private Dataset\"):\n",
    "    img = img.unsqueeze(0)\n",
    "    output = model(img)\n",
    "    pred = output.argmax(dim=1).item()\n",
    "    loss = criterion(output, torch.tensor([label]))\n",
    "    private_labels.append(label)\n",
    "    private_preds.append(pred)\n",
    "    private_losses.append(loss.item())\n",
    "\n",
    "private_accuracy = np.mean(np.array(private_preds) == np.array(private_labels))\n",
    "print(f\"Private Dataset Accuracy: {private_accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(private_losses, bins=30, color='tomato', alpha=0.7)\n",
    "plt.xlabel(\"Cross Entropy Loss\")\n",
    "plt.ylabel(\"Sample Count\")\n",
    "plt.title(\"Loss Distribution on Private Dataset\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Private Loss: {np.mean(private_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9fa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
